{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from data_prepare import samples\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '/Users/kaichen/workspace/data/models/deepseekr1-1.5b'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name).to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to datasets.jsonl\n"
     ]
    }
   ],
   "source": [
    "with open('datasets.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for sample in samples:\n",
    "        json_line = json.dumps(sample, ensure_ascii=False)\n",
    "        f.write(json_line + '\\n')\n",
    "    else:\n",
    "        print('Data saved to datasets.jsonl')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 50 examples [00:00, 1283.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files={\"train\": \"datasets.jsonl\"}, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "è¯¥å‡½æ•°çš„ä½œç”¨æ˜¯å°†æ•°æ®é›†ä¸­çš„æ ·æœ¬è½¬æ¢ä¸ºæ¨¡å‹è®­ç»ƒæ‰€éœ€çš„æ ¼å¼ï¼Œä¸»è¦åŒ…æ‹¬æ–‡æœ¬åˆå¹¶ã€åˆ†è¯å¤„ç†åŠæ ‡ç­¾ç”Ÿæˆã€‚ä»¥ä¸‹æ˜¯è¯¦ç»†è§£é‡Šï¼š\n",
    "\n",
    "### å‡½æ•°åŠŸèƒ½åˆ†è§£\n",
    "1. **æ–‡æœ¬åˆå¹¶**ï¼š\n",
    "   ```python\n",
    "   text = [f\"{prompt}\\n{completion}\" for prompt, completion in zip(examples['prompt'], examples['completion'])]\n",
    "   ```\n",
    "   - å°†æ¯ä¸ªæ ·æœ¬çš„ `prompt`ï¼ˆè¾“å…¥æç¤ºï¼‰å’Œ `completion`ï¼ˆé¢„æœŸè¾“å‡ºï¼‰é€šè¿‡æ¢è¡Œç¬¦ `\\n` æ‹¼æ¥æˆä¸€ä¸ªå®Œæ•´æ–‡æœ¬ã€‚\n",
    "   - ä¾‹å¦‚ï¼šè‹¥ `prompt` ä¸ºâ€œå†™ä¸€é¦–è¯—â€ï¼Œ`completion` ä¸ºâ€œæ˜¥çœ ä¸è§‰æ™“â€ï¼Œåˆå¹¶åä¸ºâ€œå†™ä¸€é¦–è¯—\\næ˜¥çœ ä¸è§‰æ™“â€ã€‚\n",
    "\n",
    "2. **åˆ†è¯å¤„ç†**ï¼š\n",
    "   ```python\n",
    "   tokens = tokenizer(text, padding=\"max_length\", truncation=True, max_length=512)\n",
    "   ```\n",
    "   - ä½¿ç”¨é¢„è®­ç»ƒçš„ `tokenizer` å¯¹åˆå¹¶åçš„æ–‡æœ¬è¿›è¡Œåˆ†è¯ã€‚\n",
    "   - **å¡«å…… (Padding)**ï¼šå°†æ‰€æœ‰åºåˆ—å¡«å……åˆ°å›ºå®šé•¿åº¦ï¼ˆ`max_length=512`ï¼‰ï¼Œç¡®ä¿æ‰¹é‡è¾“å…¥æ—¶å½¢çŠ¶ç»Ÿä¸€ã€‚\n",
    "   - **æˆªæ–­ (Truncation)**ï¼šè‹¥æ–‡æœ¬è¶…è¿‡ 512 ä¸ª Tokenï¼Œè‡ªåŠ¨æˆªæ–­ä»¥é€‚é…æ¨¡å‹æœ€å¤§é•¿åº¦é™åˆ¶ã€‚\n",
    "\n",
    "3. **ç”Ÿæˆæ ‡ç­¾**ï¼š\n",
    "   ```python\n",
    "   tokens['labels'] = tokens['input_ids'].copy()\n",
    "   ```\n",
    "   - å°†åˆ†è¯åçš„ `input_ids`ï¼ˆToken ID åºåˆ—ï¼‰ç›´æ¥å¤åˆ¶ä¸º `labels`ï¼Œç”¨äºç›‘ç£å­¦ä¹ ã€‚\n",
    "   - åœ¨è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ GPTï¼‰ä¸­ï¼Œæ¨¡å‹çš„ç›®æ ‡æ˜¯æ ¹æ®ä¸Šæ–‡é¢„æµ‹ä¸‹ä¸€ä¸ª Tokenï¼Œæ­¤æ—¶æ ‡ç­¾åº”ä¸è¾“å…¥åºåˆ—ä¸€è‡´ï¼Œä½†å®é™…è®­ç»ƒæ—¶éœ€åœ¨æ¨¡å‹å†…éƒ¨å°†æ ‡ç­¾å³ç§»ä¸€ä½ã€‚\n",
    "\n",
    "### ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªå‡½æ•°ï¼Ÿ\n",
    "1. **æ•°æ®æ ¼å¼åŒ–**ï¼š\n",
    "   - æ¨¡å‹æ— æ³•ç›´æ¥å¤„ç†åŸå§‹æ–‡æœ¬ï¼Œéœ€è½¬æ¢ä¸º Token ID åºåˆ—ã€‚æ­¤å‡½æ•°ç»Ÿä¸€äº†æ•°æ®æ ¼å¼ï¼Œç¡®ä¿æ‰€æœ‰æ ·æœ¬ç¬¦åˆæ¨¡å‹è¾“å…¥è¦æ±‚ã€‚\n",
    "\n",
    "2. **é•¿åº¦æ§åˆ¶**ï¼š\n",
    "   - é€šè¿‡å¡«å……å’Œæˆªæ–­ï¼Œå¤„ç†ä¸åŒé•¿åº¦çš„æ–‡æœ¬ï¼Œé¿å…è®­ç»ƒæ—¶å› åºåˆ—é•¿åº¦ä¸ä¸€è‡´å¯¼è‡´çš„é”™è¯¯ã€‚\n",
    "\n",
    "3. **æ ‡ç­¾ç”Ÿæˆ**ï¼š\n",
    "   - ä¸ºç›‘ç£å­¦ä¹ æä¾›ç›®æ ‡æ ‡ç­¾ã€‚åœ¨ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæ ‡ç­¾å¸®åŠ©æ¨¡å‹å­¦ä¹ å¦‚ä½•ä» `prompt` ç”Ÿæˆæ­£ç¡®çš„ `completion`ã€‚\n",
    "\n",
    "### æ½œåœ¨é—®é¢˜ä¸æ”¹è¿›\n",
    "- **æ ‡ç­¾é®ç›–**ï¼š\n",
    "  - **é—®é¢˜**ï¼šå½“å‰å‡½æ•°æœªåŒºåˆ† `prompt` å’Œ `completion` éƒ¨åˆ†çš„æ ‡ç­¾ï¼Œå¯¼è‡´æ¨¡å‹å¯èƒ½å­¦ä¹ é¢„æµ‹ `prompt` æœ¬èº«ï¼Œè€Œéä»…ç”Ÿæˆ `completion`ã€‚\n",
    "  - **æ”¹è¿›**ï¼šè‹¥éœ€æ¨¡å‹ä»…å…³æ³¨ `completion` çš„ç”Ÿæˆï¼Œåº”å°† `prompt` éƒ¨åˆ†çš„æ ‡ç­¾è®¾ä¸º `-100`ï¼ˆæŸå¤±è®¡ç®—æ—¶å¿½ç•¥ï¼‰ã€‚ä¾‹å¦‚ï¼š\n",
    "    ```python\n",
    "    prompt_tokens = tokenizer(examples['prompt'], add_special_tokens=False)\n",
    "    prompt_length = len(prompt_tokens['input_ids']) + 1  # +1 ä¸ºæ¢è¡Œç¬¦\n",
    "    labels = [\n",
    "        [-100] * prompt_length + input_ids[prompt_length:] \n",
    "        for input_ids in tokens['input_ids']\n",
    "    ]\n",
    "    tokens['labels'] = labels\n",
    "    ```\n",
    "\n",
    "### é€‚ç”¨åœºæ™¯\n",
    "- **é¢„è®­ç»ƒä»»åŠ¡**ï¼šæ¨¡å‹éœ€å­¦ä¹ å®Œæ•´æ–‡æœ¬çš„åˆ†å¸ƒï¼ˆåŒ…æ‹¬ `prompt` å’Œ `completion`ï¼‰ã€‚\n",
    "- **ç”Ÿæˆä»»åŠ¡ï¼ˆéœ€è°ƒæ•´ï¼‰**ï¼šè‹¥éœ€æ¨¡å‹æ ¹æ® `prompt` ç”Ÿæˆ `completion`ï¼Œéœ€ä¿®æ”¹æ ‡ç­¾ä»¥å±è”½ `prompt` éƒ¨åˆ†çš„æŸå¤±è®¡ç®—ã€‚\n",
    "\n",
    "### æ€»ç»“\n",
    "è¯¥å‡½æ•°æ˜¯æ•°æ®é¢„å¤„ç†çš„æ ¸å¿ƒæ­¥éª¤ï¼Œè´Ÿè´£å°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯è®­ç»ƒçš„æ ¼å¼ï¼Œä½†åœ¨å®é™…ä»»åŠ¡ä¸­å¯èƒ½éœ€è¦æ ¹æ®è®­ç»ƒç›®æ ‡è°ƒæ•´æ ‡ç­¾ç”Ÿæˆé€»è¾‘ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_function(examples, tokenizer):\n",
    "    text = [f\"{prompt}\\n{completion}\" for prompt, completion in zip(examples['prompt'], examples['completion'])]\n",
    "    tokens = tokenizer(text, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    tokens['labels'] = tokens['input_ids'].copy()\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [00:00<00:00, 1484.62 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 1545.09 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(lambda examples: tokenizer_function(examples, tokenizer), batched=True)\n",
    "tokenized_eval_dataset = test_dataset.map(lambda examples: tokenizer_function(examples, tokenizer), batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Question 28: Why is it important to practice dynamics in singing?', 'completion': 'Answer 28: Dynamics add emotion and variety to your performance, making your singing more expressive and engaging.', 'input_ids': [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151646, 14582, 220, 17, 23, 25, 8429, 374, 432, 2989, 311, 6588, 29195, 304, 25083, 5267, 16141, 220, 17, 23, 25, 52611, 912, 19772, 323, 8045, 311, 697, 5068, 11, 3259, 697, 25083, 803, 77123, 323, 22570, 13], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151646, 14582, 220, 17, 23, 25, 8429, 374, 432, 2989, 311, 6588, 29195, 304, 25083, 5267, 16141, 220, 17, 23, 25, 52611, 912, 19772, 323, 8045, 311, 697, 5068, 11, 3259, 697, 25083, 803, 77123, 323, 22570, 13]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization\n",
    "\n",
    "GPU required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BitsAndBytesConfig` çš„è¯¦ç»†è§£é‡Š\n",
    "\n",
    "#### **ä½œç”¨**\n",
    "`BitsAndBytesConfig` æ˜¯ Hugging Face Transformers åº“ä¸­ç”¨äºé…ç½® **æ¨¡å‹é‡åŒ–ï¼ˆQuantizationï¼‰** çš„ç±»ï¼Œå…¶æ ¸å¿ƒç›®æ ‡æ˜¯é€šè¿‡é™ä½æ¨¡å‹å‚æ•°çš„ç²¾åº¦ï¼ˆå¦‚å°† 32 ä½æµ®ç‚¹æ•°è½¬æ¢ä¸º 8 ä½æˆ– 4 ä½æ•´æ•°ï¼‰ï¼Œæ˜¾è‘—å‡å°‘æ¨¡å‹çš„å†…å­˜å ç”¨ï¼Œä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ LLaMAã€GPTï¼‰èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„è®¾å¤‡ï¼ˆå¦‚æ¶ˆè´¹çº§ GPU æˆ– CPUï¼‰ä¸Šè¿è¡Œã€‚\n",
    "\n",
    "#### **æ ¸å¿ƒåŠŸèƒ½**\n",
    "1. **å†…å­˜ä¼˜åŒ–**ï¼š\n",
    "   - å°†æ¨¡å‹æƒé‡ä» `float32` è½¬æ¢ä¸º `int8` æˆ– `int4`ï¼Œå†…å­˜å ç”¨å‡å°‘è‡³åŸæ¥çš„ 1/4 æˆ– 1/8ã€‚\n",
    "   - ä¾‹å¦‚ï¼šä¸€ä¸ª 10GB çš„æ¨¡å‹ï¼Œ8 ä½é‡åŒ–åä»…éœ€çº¦ 2.5GBï¼Œ4 ä½é‡åŒ–ä»…éœ€çº¦ 1.25GBã€‚\n",
    "\n",
    "2. **æ¨ç†åŠ é€Ÿ**ï¼š\n",
    "   - é‡åŒ–åçš„æ¨¡å‹åœ¨æ”¯æŒä½ç²¾åº¦è®¡ç®—çš„ç¡¬ä»¶ï¼ˆå¦‚ NVIDIA GPUï¼‰ä¸Šå¯èƒ½æ›´å¿«ï¼Œä½†éœ€æƒè¡¡ç²¾åº¦æŸå¤±ã€‚\n",
    "\n",
    "3. **çµæ´»é…ç½®**ï¼š\n",
    "   - æ”¯æŒæ··åˆç²¾åº¦ï¼ˆéƒ¨åˆ†å±‚ä¿ç•™é«˜ç²¾åº¦ï¼‰ã€é˜ˆå€¼æ§åˆ¶ï¼ˆè·³è¿‡å°æ•°å€¼çš„é‡åŒ–ï¼‰ç­‰é«˜çº§è®¾ç½®ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### **å‚æ•°è¯¦è§£**\n",
    "ä»¥ä¸‹æ˜¯ `BitsAndBytesConfig` çš„å…³é”®å‚æ•°ï¼ˆä»¥ 8 ä½å’Œ 4 ä½é‡åŒ–ä¸ºé‡ç‚¹ï¼‰ï¼š\n",
    "\n",
    "| å‚æ•° | ç±»å‹ | ä½œç”¨ | ç¤ºä¾‹å€¼ |\n",
    "|------|------|------|--------|\n",
    "| `load_in_8bit` | `bool` | å¯ç”¨ 8 ä½é‡åŒ– | `True` |\n",
    "| `load_in_4bit` | `bool` | å¯ç”¨ 4 ä½é‡åŒ– | `True` |\n",
    "| `llm_int8_threshold` | `float` | é˜ˆå€¼ï¼šè¶…è¿‡æ­¤å€¼çš„å‚æ•°ä¿ç•™ä¸ºæµ®ç‚¹ | `6.0` |\n",
    "| `bnb_4bit_quant_type` | `str` | 4 ä½é‡åŒ–çš„ç±»å‹ï¼ˆ`\"fp4\"` æˆ– `\"nf4\"`ï¼‰ | `\"nf4\"` |\n",
    "| `bnb_4bit_compute_dtype` | `torch.dtype` | 4 ä½é‡åŒ–çš„è®¡ç®—æ•°æ®ç±»å‹ | `torch.float16` |\n",
    "| `bnb_4bit_use_double_quant` | `bool` | æ˜¯å¦å¯ç”¨åŒé‡é‡åŒ–ï¼ˆè¿›ä¸€æ­¥å‹ç¼©ï¼‰ | `True` |\n",
    "\n",
    "---\n",
    "\n",
    "### **å®Œæ•´ç”¨æ³•ç¤ºä¾‹**\n",
    "\n",
    "#### **1. 8 ä½é‡åŒ–**\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# é…ç½® 8 ä½é‡åŒ–ï¼Œå¹¶è®¾ç½®é‡åŒ–é˜ˆå€¼\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0  # ç»å¯¹å€¼è¶…è¿‡ 6.0 çš„å‚æ•°ä¿ç•™ä¸ºæµ®ç‚¹\n",
    ")\n",
    "\n",
    "# åŠ è½½é‡åŒ–åçš„æ¨¡å‹\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"  # è‡ªåŠ¨åˆ†é…æ¨¡å‹å±‚åˆ°å¯ç”¨è®¾å¤‡ï¼ˆGPU/CPUï¼‰\n",
    ")\n",
    "```\n",
    "\n",
    "#### **2. 4 ä½é‡åŒ–ï¼ˆæ›´é«˜æ•ˆï¼‰**\n",
    "```python\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",          # ä½¿ç”¨ 4 ä½ NormalFloat é‡åŒ–\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # è®¡ç®—æ—¶ä½¿ç”¨ float16 åŠ é€Ÿ\n",
    "    bnb_4bit_use_double_quant=True      # å¯ç”¨åŒé‡é‡åŒ–ï¼ˆäºŒæ¬¡å‹ç¼©ï¼‰\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-v0.1\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **ä½¿ç”¨åœºæ™¯**\n",
    "| åœºæ™¯ | æ¨èé…ç½® | è¯´æ˜ |\n",
    "|------|----------|------|\n",
    "| ä½æ˜¾å­˜ GPU æ¨ç† | `load_in_8bit=True` | å¹³è¡¡å†…å­˜å’Œç²¾åº¦ |\n",
    "| è¶…å¤§è§„æ¨¡æ¨¡å‹éƒ¨ç½² | `load_in_4bit=True` + åŒé‡é‡åŒ– | æè‡´å‹ç¼©å†…å­˜ |\n",
    "| é«˜ç²¾åº¦éœ€æ±‚åœºæ™¯ | `llm_int8_threshold=6.0` | ä¿ç•™é‡è¦å‚æ•°ä¸ºæµ®ç‚¹ |\n",
    "\n",
    "---\n",
    "\n",
    "### **æ³¨æ„äº‹é¡¹**\n",
    "1. **ä¾èµ–å®‰è£…**ï¼š\n",
    "   ```bash\n",
    "   pip install bitsandbytes  # å¿…é¡»å®‰è£…é‡åŒ–æ”¯æŒåº“\n",
    "   pip install accelerate     # ç”¨äºè®¾å¤‡è‡ªåŠ¨åˆ†é…ï¼ˆdevice_map=\"auto\"ï¼‰\n",
    "   ```\n",
    "\n",
    "2. **æ€§èƒ½æƒè¡¡**ï¼š\n",
    "   - é‡åŒ–å¯èƒ½å¯¼è‡´æ¨¡å‹ç²¾åº¦ä¸‹é™ï¼Œå°¤å…¶åœ¨ç”Ÿæˆä»»åŠ¡ä¸­å¯èƒ½å½±å“æ–‡æœ¬è¿è´¯æ€§ã€‚\n",
    "   - 4 ä½é‡åŒ–æ¯” 8 ä½æ›´æ¿€è¿›ï¼Œå†…å­˜æ›´å°ï¼Œä½†ç²¾åº¦æŸå¤±æ›´å¤§ã€‚\n",
    "\n",
    "3. **ç¡¬ä»¶å…¼å®¹æ€§**ï¼š\n",
    "   - é‡åŒ–æ¨¡å‹åœ¨ NVIDIA GPU ä¸Šæ”¯æŒæœ€ä½³ï¼Œéƒ¨åˆ†æ“ä½œåœ¨ CPU ä¸Šå¯èƒ½æ— æ³•åŠ é€Ÿã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### **å®Œæ•´æ€»ç»“**\n",
    "`BitsAndBytesConfig` æ˜¯ Transformers åº“ä¸­å®ç°æ¨¡å‹é‡åŒ–çš„æ ¸å¿ƒé…ç½®ç±»ï¼Œé€šè¿‡å°†æ¨¡å‹æƒé‡ä»é«˜ç²¾åº¦æµ®ç‚¹æ•°è½¬æ¢ä¸ºä½ç²¾åº¦æ•´æ•°ï¼ˆ8 ä½æˆ– 4 ä½ï¼‰ï¼Œæ˜¾è‘—é™ä½å†…å­˜å ç”¨ï¼Œä½¿å¤§æ¨¡å‹èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­è¿è¡Œã€‚å…¶å…³é”®åŠŸèƒ½åŒ…æ‹¬ï¼š\n",
    "\n",
    "1. **å†…å­˜å‹ç¼©**ï¼š8 ä½é‡åŒ–å‡å°‘å†…å­˜è‡³ 1/4ï¼Œ4 ä½é‡åŒ–è‡³ 1/8ã€‚\n",
    "2. **çµæ´»é…ç½®**ï¼šæ”¯æŒé˜ˆå€¼æ§åˆ¶ã€æ··åˆç²¾åº¦ã€åŒé‡é‡åŒ–ç­‰é«˜çº§é€‰é¡¹ã€‚\n",
    "3. **æ˜“ç”¨æ€§**ï¼šåªéœ€åœ¨åŠ è½½æ¨¡å‹æ—¶ä¼ å…¥é…ç½®ï¼Œæ— éœ€ä¿®æ”¹æ¨¡å‹ä»£ç ã€‚\n",
    "\n",
    "**ç¤ºä¾‹æ€»ç»“**ï¼š\n",
    "- **8 ä½é‡åŒ–**ï¼šé€‚åˆå¤§å¤šæ•°åœºæ™¯ï¼Œå¹³è¡¡å†…å­˜å’Œç²¾åº¦ã€‚\n",
    "- **4 ä½é‡åŒ–**ï¼šé€‚åˆæè‡´å†…å­˜ä¼˜åŒ–ï¼Œéœ€æ­é… `nf4` ç±»å‹å’Œ `float16` è®¡ç®—ä»¥ç»´æŒæ€§èƒ½ã€‚\n",
    "\n",
    "é€šè¿‡åˆç†é…ç½® `BitsAndBytesConfig`ï¼Œå¼€å‘è€…å¯ä»¥åœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šé«˜æ•ˆéƒ¨ç½²ç™¾äº¿å‚æ•°çº§åˆ«çš„è¯­è¨€æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "# device_map={\"cuda:0\": \"cpu\"}\n",
    "# device_map={\"auto\"}\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, config=quantization_config)\n",
    "# model.save_pretrained(model_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lora \n",
    "\n",
    "GPU required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **PEFT åº“ä¸ LoRA æ–¹æ³•è¯¦è§£**\n",
    "\n",
    "#### **PEFT åº“ç®€ä»‹**\n",
    "**PEFT**ï¼ˆParameter-Efficient Fine-Tuningï¼‰æ˜¯ Hugging Face æ¨å‡ºçš„åº“ï¼Œæ—¨åœ¨é€šè¿‡é«˜æ•ˆè°ƒæ•´å°‘é‡å‚æ•°æ¥å¾®è°ƒå¤§å‹é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚ GPTã€LLaMAï¼‰ã€‚å…¶æ ¸å¿ƒç›®æ ‡æ˜¯ **é™ä½è®­ç»ƒæˆæœ¬**ï¼ˆæ˜¾å­˜ã€ç®—åŠ›ï¼‰ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚å¸¸è§åº”ç”¨åœºæ™¯åŒ…æ‹¬ï¼š\n",
    "- **èµ„æºå—é™ç¯å¢ƒ**ï¼šåœ¨æ¶ˆè´¹çº§ GPUï¼ˆå¦‚ 24GB æ˜¾å­˜ï¼‰ä¸Šå¾®è°ƒ 10B+ å‚æ•°çš„æ¨¡å‹ã€‚\n",
    "- **å¤šä»»åŠ¡é€‚é…**ï¼šä¸ºä¸åŒä»»åŠ¡ç”Ÿæˆè½»é‡çº§é€‚é…å™¨ï¼Œæ— éœ€å­˜å‚¨å®Œæ•´æ¨¡å‹å‰¯æœ¬ã€‚\n",
    "- **é¿å…ç¾éš¾æ€§é—å¿˜**ï¼šä»…ä¿®æ”¹éƒ¨åˆ†å‚æ•°ï¼Œä¿ç•™é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€èƒ½åŠ›ã€‚\n",
    "\n",
    "æ”¯æŒçš„é«˜æ•ˆå¾®è°ƒæ–¹æ³•åŒ…æ‹¬ï¼š\n",
    "- **LoRA**ï¼ˆLow-Rank Adaptationï¼Œä½ç§©é€‚åº”ï¼‰\n",
    "- **Adapter**ï¼ˆæ’å…¥å°å‹ç¥ç»ç½‘ç»œæ¨¡å—ï¼‰\n",
    "- **Prompt Tuning**ï¼ˆå­¦ä¹ è½¯æç¤ºå‘é‡ï¼‰\n",
    "- **IA3**ï¼ˆé€šè¿‡å‘é‡ç¼©æ”¾æ¿€æ´»å€¼ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### **LoRA æ–¹æ³•åŸç†**\n",
    "LoRA çš„æ ¸å¿ƒæ€æƒ³æ˜¯ **å†»ç»“åŸå§‹æ¨¡å‹æƒé‡**ï¼Œä»…é€šè¿‡ä½ç§©çŸ©é˜µï¼ˆLow-Rank Matricesï¼‰å¯¹æƒé‡æ›´æ–°è¿›è¡Œå‚æ•°åŒ–ã€‚å…·ä½“æ¥è¯´ï¼š\n",
    "1. å¯¹äºåŸå§‹æƒé‡çŸ©é˜µ \\( W \\in \\mathbb{R}^{d \\times k} \\)ï¼Œå®šä¹‰ä¸¤ä¸ªä½ç§©çŸ©é˜µ \\( A \\in \\mathbb{R}^{d \\times r} \\) å’Œ \\( B \\in \\mathbb{R}^{r \\times k} \\)ï¼Œå…¶ä¸­ \\( r \\ll \\min(d,k) \\)ã€‚\n",
    "2. æƒé‡æ›´æ–°é‡è¡¨ç¤ºä¸º \\( \\Delta W = A \\cdot B \\)ï¼Œæœ€ç»ˆè¾“å‡ºä¸º \\( W + \\Delta W \\)ã€‚\n",
    "3. è®­ç»ƒæ—¶ä»…æ›´æ–° \\( A \\) å’Œ \\( B \\)ï¼ŒåŸå§‹ \\( W \\) ä¿æŒå†»ç»“ã€‚\n",
    "\n",
    "![LoRA ç¤ºæ„å›¾](https://miro.medium.com/v2/resize:fit:720/format:webp/1*_0fYSlZRlC8mAJi4Z3k1GQ.png)\n",
    "\n",
    "---\n",
    "\n",
    "### **`LoraConfig` å‚æ•°è¯¦è§£**\n",
    "ä»¥ä¸‹æ˜¯ `LoraConfig` çš„æ‰€æœ‰å‚æ•°åŠå…¶ä½œç”¨ï¼š\n",
    "\n",
    "| å‚æ•° | ç±»å‹ | ä½œç”¨ | å½±å“ | ç¤ºä¾‹å€¼ |\n",
    "|------|------|------|------|--------|\n",
    "| **`r`** | `int` | ä½ç§©çŸ©é˜µçš„ç§©ï¼ˆRankï¼‰ | ç§©è¶Šå¤§ï¼Œå¯è®­ç»ƒå‚æ•°è¶Šå¤šï¼Œçµæ´»æ€§å¢åŠ ï¼Œä½†å¯èƒ½è¿‡æ‹Ÿåˆ | `8`, `16`, `32` |\n",
    "| **`lora_alpha`** | `int` | ä½ç§©é€‚åº”çš„ç¼©æ”¾å› å­ | æ§åˆ¶ä½ç§©çŸ©é˜µçš„æƒé‡å¼ºåº¦ï¼Œé€šå¸¸ä¸ `r` æˆæ¯”ä¾‹ | `16`, `32`, `64` |\n",
    "| **`lora_dropout`** | `float` | LoRA å±‚çš„ Dropout ç‡ | æ­£åˆ™åŒ–å¼ºåº¦ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ | `0.1`, `0.2` |\n",
    "| **`task_type`** | `TaskType` | ä»»åŠ¡ç±»å‹ | å†³å®šæ¨¡å‹ç»“æ„è°ƒæ•´æ–¹å¼ | `CAUSAL_LM`, `SEQ_CLS` |\n",
    "| **`target_modules`** | `List[str]` | åº”ç”¨ LoRA çš„æ¨¡å—åˆ—è¡¨ | æŒ‡å®šå¯¹å“ªäº›å±‚è¿›è¡Œä½ç§©é€‚åº” | `[\"q_proj\", \"v_proj\"]` |\n",
    "| **`bias`** | `str` | æ˜¯å¦è®­ç»ƒåç½®é¡¹ | æ§åˆ¶æ˜¯å¦æ›´æ–°åç½®å‚æ•° | `\"none\"`, `\"all\"`, `\"lora_only\"` |\n",
    "| **`modules_to_save`** | `List[str]` | é¢å¤–è®­ç»ƒçš„æ¨¡å—ï¼ˆé LoRA å±‚ï¼‰ | å…è®¸ç‰¹å®šæ¨¡å—å®Œå…¨å‚ä¸è®­ç»ƒ | `[\"lm_head\"]` |\n",
    "| **`layers_to_transform`** | `List[int]` | åº”ç”¨ LoRA çš„å±‚ç´¢å¼• | ä»…å¯¹æŒ‡å®šå±‚è¿›è¡Œä½ç§©é€‚åº” | `[0, 2, 4]` |\n",
    "| **`layers_pattern`** | `str` | å±‚åç§°åŒ¹é…æ¨¡å¼ | é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼é€‰æ‹©ç›®æ ‡å±‚ | `\".*decoder.*\"` |\n",
    "\n",
    "---\n",
    "\n",
    "#### **å…³é”®å‚æ•°è§£é‡Š**\n",
    "1. **`r`ï¼ˆRankï¼‰**:\n",
    "   - **ä½œç”¨**ï¼šä½ç§©çŸ©é˜µçš„ç»´åº¦ï¼Œç›´æ¥å½±å“å¯è®­ç»ƒå‚æ•°æ•°é‡ã€‚\n",
    "   - **ç¤ºä¾‹**ï¼šè‹¥ `r=8`ï¼Œæ¯ä¸ª LoRA å±‚çš„å‚æ•°é‡ä¸º \\( d \\times 8 + 8 \\times k \\)ã€‚\n",
    "   - **å»ºè®®**ï¼šä»è¾ƒå°çš„å€¼ï¼ˆå¦‚ `8`ï¼‰å¼€å§‹ï¼Œé€æ­¥å¢åŠ ç›´åˆ°æ€§èƒ½ç¨³å®šã€‚\n",
    "\n",
    "2. **`lora_alpha`**:\n",
    "   - **ä½œç”¨**ï¼šç¼©æ”¾ä½ç§©çŸ©é˜µçš„æƒé‡ï¼Œæœ€ç»ˆæ›´æ–°é‡ä¸º \\( \\Delta W = \\frac{\\alpha}{r} \\cdot A \\cdot B \\)ã€‚\n",
    "   - **ç¤ºä¾‹**ï¼šè‹¥ `alpha=32`, `r=16`ï¼Œåˆ™ç¼©æ”¾å› å­ä¸º `2`ã€‚\n",
    "   - **å»ºè®®**ï¼šé€šå¸¸è®¾ç½®ä¸º `r` çš„ 2~4 å€ï¼Œå¦‚ `r=8` â†’ `alpha=16`ã€‚\n",
    "\n",
    "3. **`target_modules`**:\n",
    "   - **ä½œç”¨**ï¼šæŒ‡å®šæ¨¡å‹ä¸­å¯¹å“ªäº›çº¿æ€§å±‚ï¼ˆå¦‚æ³¨æ„åŠ›æ¨¡å—çš„ `q_proj`, `v_proj`ï¼‰åº”ç”¨ LoRAã€‚\n",
    "   - **ç¤ºä¾‹**ï¼šå¯¹ LLaMA æ¨¡å‹ï¼Œé€šå¸¸é€‰æ‹© `[\"q_proj\", \"v_proj\"]`ã€‚\n",
    "   - **å»ºè®®**ï¼šå‚è€ƒæ¨¡å‹æ¶æ„æ–‡æ¡£ï¼Œé€‰æ‹©ä¸ä»»åŠ¡ç›¸å…³çš„æ¨¡å—ã€‚\n",
    "\n",
    "4. **`modules_to_save`**:\n",
    "   - **ä½œç”¨**ï¼šå…è®¸æŸäº›æ¨¡å—ï¼ˆå¦‚åˆ†ç±»å¤´ `lm_head`ï¼‰åœ¨å¾®è°ƒæ—¶å®Œå…¨è®­ç»ƒï¼ˆé LoRA æ¨¡å¼ï¼‰ã€‚\n",
    "   - **ç¤ºä¾‹**ï¼šåœ¨ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œé€šå¸¸éœ€è¦å¾®è°ƒè¾“å‡ºå±‚ï¼š`modules_to_save=[\"lm_head\"]`ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### **å®Œæ•´ä»£ç ç¤ºä¾‹**\n",
    "```python\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "\n",
    "# é…ç½® LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                          # ä½ç§©çŸ©é˜µçš„ç§©\n",
    "    lora_alpha=32,                 # ç¼©æ”¾å› å­ (alpha/r å†³å®šå®é™…ç¼©æ”¾æ¯”ä¾‹)\n",
    "    lora_dropout=0.1,              # Dropout ç‡\n",
    "    task_type=TaskType.CAUSAL_LM,   # ä»»åŠ¡ç±»å‹ï¼ˆå› æœè¯­è¨€æ¨¡å‹ï¼‰\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # ç›®æ ‡æ¨¡å—ï¼ˆæŸ¥è¯¢å’Œå€¼æŠ•å½±å±‚ï¼‰\n",
    "    bias=\"none\",                   # ä¸è®­ç»ƒåç½®é¡¹\n",
    "    modules_to_save=[\"lm_head\"],   # é¢å¤–è®­ç»ƒè¾“å‡ºå±‚\n",
    ")\n",
    "\n",
    "# åº”ç”¨ LoRA åˆ°æ¨¡å‹\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# æ‰“å°å¯è®­ç»ƒå‚æ•°å æ¯”\n",
    "model.print_trainable_parameters()\n",
    "# è¾“å‡ºç¤ºä¾‹: trainable params: 8,847,360 || all params: 6,775,656,448 || 0.13%\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **é€šä¿—æ€»ç»“**\n",
    "1. **PEFT åº“**ï¼šè®©ä½ ç”¨â€œå°ä¿®å°è¡¥â€çš„æ–¹å¼å¾®è°ƒå¤§æ¨¡å‹ï¼Œæ— éœ€åŠ¨å…¨éƒ¨å‚æ•°ï¼Œçœé’±çœåŠ›ã€‚\n",
    "2. **LoRA æ–¹æ³•**ï¼šé€šè¿‡ç»™æ¨¡å‹â€œæ‰“è¡¥ä¸â€ï¼ˆä½ç§©çŸ©é˜µï¼‰ï¼Œåªè®­ç»ƒè¿™äº›è¡¥ä¸æ¥é€‚åº”æ–°ä»»åŠ¡ã€‚\n",
    "   - **`r`**ï¼šè¡¥ä¸çš„å¤§å°ï¼Œè¶Šå¤§è¶Šçµæ´»ï¼Œä½†ä¹Ÿå¯èƒ½â€œè¡¥è¿‡å¤´â€ã€‚\n",
    "   - **`alpha`**ï¼šè¡¥ä¸çš„å¼ºåº¦ï¼Œå’Œ `r` é…åˆä½¿ç”¨ï¼ˆé€šå¸¸ `alpha=2*r`ï¼‰ã€‚\n",
    "   - **`target_modules`**ï¼šå†³å®šè¡¥ä¸æ‰“åœ¨å“ªé‡Œï¼ˆå¦‚æ³¨æ„åŠ›å±‚çš„ç‰¹å®šä½ç½®ï¼‰ã€‚\n",
    "3. **å‚æ•°é€‰æ‹©**ï¼š\n",
    "   - ä» `r=8`ã€`alpha=16` å¼€å§‹ï¼Œé€æ­¥è°ƒæ•´ã€‚\n",
    "   - æ ¹æ®ä»»åŠ¡ç±»å‹ï¼ˆå¦‚æ–‡æœ¬ç”Ÿæˆã€åˆ†ç±»ï¼‰è®¾ç½® `task_type` å’Œ `target_modules`ã€‚\n",
    "   - å¦‚æœä»»åŠ¡éœ€è¦æ›´æ–°è¾“å‡ºå±‚ï¼ˆå¦‚åˆ†ç±»å¤´ï¼‰ï¼Œä½¿ç”¨ `modules_to_save`ã€‚\n",
    "\n",
    "é€šè¿‡åˆç†é…ç½® `LoraConfig`ï¼Œä½ å¯ä»¥åœ¨ 24GB æ˜¾å­˜çš„ GPU ä¸Šå¾®è°ƒ 70B å‚æ•°çš„æ¨¡å‹ï¼ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸‹é¢æˆ‘ä»¬ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€è¯¦ç»†ä»‹ç»ä¸€ä¸‹ LoRA åŠå…¶å„ä¸ªå‚æ•°çš„ä½œç”¨å’Œæ•ˆæœï¼Œå¹¶åœ¨æœ€åç»™å‡ºå®Œæ•´çš„æ€»ç»“ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ä¸€ã€LoRA ç®€ä»‹\n",
    "\n",
    "**LoRAï¼ˆLow-Rank Adaptationï¼‰** æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ã€‚ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•å¾€å¾€éœ€è¦æ›´æ–°æ•´ä¸ªå¤§æ¨¡å‹çš„å‚æ•°ï¼Œè€Œ LoRA çš„æ€è·¯æ˜¯ä¿æŒåŸå§‹é¢„è®­ç»ƒæ¨¡å‹å‚æ•°ä¸å˜ï¼Œä»…é€šè¿‡å¢åŠ ä¸€äº›é¢å¤–çš„ã€è¾ƒå°çš„â€œä½ç§©çŸ©é˜µâ€ï¼ˆå³å‚æ•°é‡éå¸¸å°‘çš„çŸ©é˜µï¼‰æ¥é€‚é…ç‰¹å®šä»»åŠ¡ã€‚è¿™æ ·åšæœ‰ä¸¤ä¸ªå¥½å¤„ï¼š\n",
    "\n",
    "- **èŠ‚çœè®¡ç®—èµ„æºå’Œå†…å­˜**ï¼šåªè®­ç»ƒå°‘é‡æ–°å¢å‚æ•°ï¼Œé™ä½äº†æ˜¾å­˜å’Œè®¡ç®—éœ€æ±‚ã€‚  \n",
    "- **æé«˜å¾®è°ƒæ•ˆç‡**ï¼šåœ¨å¤§æ¨¡å‹ä¸Šè¿›è¡Œå¾®è°ƒæ—¶ï¼Œåªæ›´æ–°æ–°å¢çš„å‚æ•°ï¼Œè®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼Œèµ„æºå ç”¨æ›´å°‘ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## äºŒã€LoRA çš„æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œå¾ˆå¤šæ“ä½œï¼ˆæ¯”å¦‚å…¨è¿æ¥å±‚ã€æ³¨æ„åŠ›å±‚ï¼‰éƒ½æ¶‰åŠå¤§çŸ©é˜µçš„ä¹˜æ³•è¿ç®—ã€‚LoRA çš„ä¸»è¦æ€è·¯æ˜¯å°†è¿™äº›å¤§çŸ©é˜µçš„æ›´æ–°è¡¨ç¤ºä¸ºä¸¤ä¸ªå°çŸ©é˜µçš„ä¹˜ç§¯ã€‚  \n",
    "- åŸå§‹æ¨¡å‹å‚æ•°ä¿æŒä¸å˜ï¼›  \n",
    "- å¢åŠ çš„ä½ç§©çŸ©é˜µç»è¿‡è®­ç»ƒæ¥æ•æ‰ä»»åŠ¡ç‰¹å®šçš„ä¿¡æ¯ï¼›  \n",
    "- æœ€ç»ˆæ¨¡å‹çš„è¾“å‡ºç”±åŸå§‹å›ºå®šå‚æ•°å’Œä½ç§©æ›´æ–°å…±åŒå†³å®šã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ä¸‰ã€LoraConfig ä¸­å„å‚æ•°çš„è¯¦ç»†ä»‹ç»\n",
    "\n",
    "åœ¨ä½¿ç”¨ PEFT åº“è¿›è¡Œ LoRA å¾®è°ƒæ—¶ï¼Œéœ€è¦é€šè¿‡ `LoraConfig` æ¥æŒ‡å®šç›¸å…³å‚æ•°ã€‚ä¸‹é¢æ˜¯ä¸»è¦å‚æ•°çš„è¯¦ç»†è§£é‡Šï¼š\n",
    "\n",
    "### 1. r\n",
    "- **å«ä¹‰**ï¼š  \n",
    "  `r` è¡¨ç¤ºä½ç§©åˆ†è§£æ—¶ä½ç§©çŸ©é˜µçš„ç§©ï¼ˆå³ä½ç»´ç©ºé—´çš„ç»´åº¦ï¼‰ã€‚\n",
    "  \n",
    "- **æ•ˆæœä¸å½±å“**ï¼š  \n",
    "  - **è¾ƒå¤§ r**ï¼šæ„å‘³ç€ä½ç§©çŸ©é˜µçš„ç»´åº¦è¾ƒé«˜ï¼Œå¯ä»¥æ•æ‰æ›´å¤šç»†èŠ‚ä¿¡æ¯ï¼Œæœ‰åŠ©äºæé«˜å¾®è°ƒåæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼›ä½†åŒæ—¶ä¹Ÿä¼šå¢åŠ æ–°å¢å‚æ•°çš„æ•°é‡å’Œè®¡ç®—é‡ã€‚  \n",
    "  - **è¾ƒå° r**ï¼šå‚æ•°æ›´å°‘ï¼Œè®¡ç®—å’Œå­˜å‚¨å¼€é”€å°ï¼Œä½†å¯èƒ½åœ¨é¢å¯¹å¤æ‚ä»»åŠ¡æ—¶é€‚åº”èƒ½åŠ›ä¸è¶³ã€‚\n",
    "\n",
    "### 2. lora_alpha\n",
    "- **å«ä¹‰**ï¼š  \n",
    "  `lora_alpha` æ˜¯ä¸€ä¸ªç¼©æ”¾å› å­ï¼Œç”¨æ¥è°ƒèŠ‚ä½ç§©çŸ©é˜µæ›´æ–°åœ¨æ•´ä½“æ¨¡å‹ä¸­çš„å½±å“åŠ›ã€‚\n",
    "  \n",
    "- **æ•ˆæœä¸å½±å“**ï¼š  \n",
    "  - **ä½œç”¨**ï¼šåœ¨å®é™…è®¡ç®—ä¸­ï¼ŒLoRA æ¨¡å—ç”Ÿæˆçš„æ›´æ–°é¡¹ä¼šå…ˆä¹˜ä»¥ `lora_alpha` å†ä¸åŸå§‹è¾“å‡ºç›¸åŠ ã€‚  \n",
    "  - **å½±å“**ï¼š  \n",
    "    - è¾ƒå¤§çš„ `lora_alpha` ä¼šæ”¾å¤§ä½ç§©æ›´æ–°çš„æ•ˆæœï¼Œä½¿å¾—å¾®è°ƒæ›´æ–°æ›´ä¸ºæ˜æ˜¾ï¼›  \n",
    "    - é€šå¸¸ï¼Œå®é™…å½±å“ä½“ç°åœ¨ `lora_alpha / r` çš„æ¯”ä¾‹ä¸Šï¼Œè¿™ä¸ªæ¯”ä¾‹å†³å®šäº†ä½ç§©æ›´æ–°å’ŒåŸå§‹å‚æ•°ä¹‹é—´çš„å¹³è¡¡ã€‚\n",
    "\n",
    "### 3. lora_dropout\n",
    "- **å«ä¹‰**ï¼š  \n",
    "  `lora_dropout` æŒ‡å®šäº†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹ä½ç§©æ›´æ–°åº”ç”¨ dropout çš„æ¦‚ç‡ã€‚\n",
    "  \n",
    "- **æ•ˆæœä¸å½±å“**ï¼š  \n",
    "  - **é˜²æ­¢è¿‡æ‹Ÿåˆ**ï¼šé€šè¿‡åœ¨è®­ç»ƒæ—¶éšæœºâ€œä¸¢å¼ƒâ€ä¸€éƒ¨åˆ†ä½ç§©æ›´æ–°ï¼Œå¯ä»¥å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ï¼Œé¿å…å¯¹è®­ç»ƒæ•°æ®è¿‡æ‹Ÿåˆã€‚  \n",
    "  - **å¹³è¡¡æ›´æ–°**ï¼šåˆç†è®¾ç½® dropout ç‡æœ‰åŠ©äºä½¿å¾—ä½ç§©æ›´æ–°æ›´åŠ ç¨³å®šï¼Œä½†å¦‚æœè¿‡é«˜å¯èƒ½ä¼šä½¿å¾—é€‚é…æ•ˆæœå‡å¼±ã€‚\n",
    "\n",
    "### 4. task_type\n",
    "- **å«ä¹‰**ï¼š  \n",
    "  `task_type` æŒ‡æ˜äº†å½“å‰å¾®è°ƒä»»åŠ¡çš„ç±»å‹ï¼Œå¦‚å› æœè¯­è¨€å»ºæ¨¡ï¼ˆCAUSAL_LMï¼‰ã€åºåˆ—åˆ†ç±»ï¼ˆSEQ_CLSï¼‰ã€é—®ç­”ä»»åŠ¡ç­‰ã€‚\n",
    "  \n",
    "- **æ•ˆæœä¸å½±å“**ï¼š  \n",
    "  - **æ¨¡å—é€‰æ‹©**ï¼šä¸åŒçš„ä»»åŠ¡å¯èƒ½éœ€è¦åœ¨æ¨¡å‹ä¸­æ³¨å…¥ LoRA æ¨¡å—çš„ä½ç½®ä¸åŒã€‚æŒ‡å®šä»»åŠ¡ç±»å‹åï¼ŒPEFT åº“ä¼šè‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æ¨¡å—è¿›è¡Œé€‚é…ã€‚  \n",
    "  - **è¾“å…¥è¾“å‡ºæ ¼å¼**ï¼šä»»åŠ¡ç±»å‹è¿˜èƒ½å¸®åŠ©ç¡®å®šè®­ç»ƒæ—¶çš„è¾“å…¥è¾“å‡ºã€æŸå¤±å‡½æ•°ç­‰è®¾ç½®ï¼Œä½¿å¾—å¾®è°ƒè¿‡ç¨‹æ›´ä¸ºåˆç†ã€‚\n",
    "\n",
    "### 5. å…¶ä»–å¯èƒ½å‚æ•°ï¼ˆæ ¹æ®ä¸åŒç‰ˆæœ¬å¯èƒ½å­˜åœ¨ï¼‰\n",
    "- **target_modules**ï¼š  \n",
    "  - **å«ä¹‰**ï¼šæ˜ç¡®æŒ‡å®šå“ªäº›æ¨¡å‹å±‚ï¼ˆä¾‹å¦‚æ³¨æ„åŠ›å±‚ã€å‰é¦ˆå±‚ï¼‰éœ€è¦ä½¿ç”¨ LoRA é€‚é…ã€‚  \n",
    "  - **æ•ˆæœ**ï¼šåªåœ¨éƒ¨åˆ†æ¨¡å—ä¸Šåº”ç”¨ LoRA å¯ä»¥è¿›ä¸€æ­¥é™ä½å‚æ•°é‡ï¼Œå¹¶ä¸“æ³¨äºæœ€å…³é”®çš„éƒ¨åˆ†ã€‚\n",
    "  \n",
    "- **bias**ï¼š  \n",
    "  - **å«ä¹‰**ï¼šå†³å®šæ˜¯å¦å¯¹æ¨¡å‹ä¸­çš„åç½®é¡¹ï¼ˆbiasï¼‰ä¹Ÿé‡‡ç”¨ LoRA è¿›è¡Œé€‚é…ã€‚  \n",
    "  - **æ•ˆæœ**ï¼šæ§åˆ¶æ˜¯å¦æ›´æ–°åç½®ï¼Œæœ‰æ—¶æ›´æ–°åç½®å¯èƒ½å¯¹ä»»åŠ¡æ•ˆæœæœ‰å¾®å°å½±å“ï¼Œæ ¹æ®ä»»åŠ¡éœ€æ±‚å†³å®šæ˜¯å¦æ›´æ–°ã€‚\n",
    "\n",
    "- **inference_mode**ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼š  \n",
    "  - **å«ä¹‰**ï¼šç”¨äºæŒ‡ç¤ºæ˜¯å¦åœ¨æ¨ç†æ—¶ä¹Ÿå¯ç”¨ LoRA çš„ç‰¹æ®Šå¤„ç†æ¨¡å¼ï¼Œä»¥ä¾¿ä¼˜åŒ–æ¨ç†é€Ÿåº¦ã€‚  \n",
    "  - **æ•ˆæœ**ï¼šåœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯ä»¥ç¦ç”¨æŸäº›é¢å¤–è®¡ç®—ï¼Œä»è€ŒåŠ é€Ÿæ¨¡å‹é¢„æµ‹ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## å››ã€å®Œæ•´é€šä¿—æ€»ç»“\n",
    "\n",
    "LoRA çš„æ ¸å¿ƒæ€æƒ³æ˜¯**ä¸æ”¹å˜åŸå§‹å¤§æ¨¡å‹çš„å‚æ•°**ï¼Œè€Œæ˜¯åœ¨å…³é”®å±‚ä¸Šæ·»åŠ ä¸¤ä¸ªå°çŸ©é˜µæ¥â€œé€‚é…â€ä»»åŠ¡ã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼š\n",
    "- **é«˜æ•ˆå¾®è°ƒ**ï¼šåªéœ€è¦è®­ç»ƒæ–°å¢çš„ä½ç§©çŸ©é˜µï¼Œå¤§å¤§å‡å°‘äº†å‚æ•°æ•°é‡å’Œè®¡ç®—èµ„æºéœ€æ±‚ã€‚  \n",
    "- **èŠ‚çœèµ„æº**ï¼šåœ¨æ˜¾å­˜æœ‰é™æˆ–è€…è®¡ç®—èƒ½åŠ›å—é™çš„åœºæ™¯ä¸‹ï¼Œå¯ä»¥è½»æ¾å¾®è°ƒå¤§æ¨¡å‹ã€‚  \n",
    "- **çµæ´»åº”ç”¨**ï¼šé€šè¿‡è°ƒæ•´å‚æ•°ï¼ˆå¦‚ `r`ã€`lora_alpha` å’Œ `lora_dropout`ï¼‰ï¼Œå¯ä»¥å¹³è¡¡æ¨¡å‹çš„é€‚åº”èƒ½åŠ›å’Œè®­ç»ƒèµ„æºæ¶ˆè€—ã€‚  \n",
    "- **ä»»åŠ¡é€‚é…**ï¼š`task_type` å‚æ•°å¸®åŠ©è‡ªåŠ¨é€‰æ‹©éœ€è¦è°ƒæ•´çš„æ¨¡å‹æ¨¡å—ï¼Œä½¿å¾— LoRA èƒ½å¤Ÿåœ¨ä¸åŒä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬ç”Ÿæˆã€åˆ†ç±»ã€é—®ç­”ç­‰ï¼‰ä¸­å‘æŒ¥ä½œç”¨ã€‚\n",
    "\n",
    "æ¯ä¸ªå‚æ•°éƒ½æœ‰å…¶å…·ä½“çš„ä½œç”¨ï¼š\n",
    "- **r** å†³å®šäº†æ–°å¢çŸ©é˜µçš„å¤§å°ï¼Œç›´æ¥å½±å“æ¨¡å‹èƒ½æ•æ‰çš„ä¿¡æ¯é‡å’Œè®­ç»ƒæ—¶çš„èµ„æºå ç”¨ï¼›\n",
    "- **lora_alpha** æ§åˆ¶ä½ç§©æ›´æ–°çš„å¼ºåº¦ï¼Œå¸®åŠ©å¹³è¡¡æ›´æ–°æ•ˆæœï¼›\n",
    "- **lora_dropout** é€šè¿‡éšæœºä¸¢å¼ƒéƒ¨åˆ†æ›´æ–°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆå¹¶å¢å¼ºè®­ç»ƒç¨³å®šæ€§ï¼›\n",
    "- **task_type** åˆ™å‘Šè¯‰ç³»ç»Ÿè¯¥åœ¨æ¨¡å‹çš„å“ªäº›éƒ¨åˆ†æ’å…¥ä½ç§©é€‚é…å™¨ï¼Œç¡®ä¿é€‚é…æ–¹å¼ä¸ä»»åŠ¡åŒ¹é…ã€‚\n",
    "\n",
    "æ€»ä¹‹ï¼ŒLoRA ä¸ºå¤§æ¨¡å‹çš„å¾®è°ƒæä¾›äº†ä¸€ç§æ—¢é«˜æ•ˆåˆç»æµçš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡ä»…è°ƒæ•´ä¸€å°éƒ¨åˆ†å‚æ•°ï¼Œä½¿å¾—å¤§æ¨¡å‹åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šä¹Ÿèƒ½å¿«é€Ÿé€‚åº”ï¼Œå¹¶ä¸”å¤§å¤§é™ä½äº†è®­ç»ƒèµ„æºå’Œå­˜å‚¨çš„è¦æ±‚ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,179,072 || all params: 1,779,267,072 || trainable%: 0.1225\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,  # Number of bits for the mantissa\n",
    "    lora_alpha=32,  # Scaling factor for low-rank adaptation\n",
    "    lora_dropout=0.1,  # Dropout rate for low-rank adaptation\n",
    "    task_type=TaskType.CAUSAL_LM,  # Task type\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "# model.save_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train parameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nä¸‹é¢å¯¹æ¯ä¸ªå‚æ•°è¿›è¡Œç®€æ˜é€šä¿—çš„è§£é‡Šï¼š\\n\\n- **output_dir**ï¼šæŒ‡å®šè®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜å¾®è°ƒæ¨¡å‹ã€æ£€æŸ¥ç‚¹ï¼ˆcheckpointï¼‰ç­‰è¾“å‡ºæ–‡ä»¶çš„ç›®å½•ã€‚  \\n- **num_train_epochs**ï¼šæ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­éå†è®­ç»ƒæ•°æ®é›†çš„æ€»è½®æ•°ã€‚  \\n- **per_device_train_batch_size**ï¼šæ¯ä¸ªè®¾å¤‡ï¼ˆä¾‹å¦‚æ¯ä¸ª GPUï¼‰ä¸Šä¸€æ¬¡è®­ç»ƒä½¿ç”¨çš„æ ·æœ¬æ•°ã€‚å¦‚æœæœ‰å¤šä¸ªè®¾å¤‡ï¼Œå®é™…çš„æ‰¹é‡å¤§å°ä¼šä¹˜ä»¥è®¾å¤‡æ•°é‡ã€‚  \\n- **gradient_accumulation_steps**ï¼šæ¯ç´¯è®¡å¤šå°‘æ­¥æ¢¯åº¦åå†è¿›è¡Œä¸€æ¬¡å‚æ•°æ›´æ–°ã€‚è¿™ç›¸å½“äºå¢å¤§äº†æ‰¹é‡å¤§å°ï¼Œæœ‰åŠ©äºåœ¨æ˜¾å­˜è¾ƒå°çš„è®¾å¤‡ä¸Šè®­ç»ƒå¤§æ¨¡å‹ã€‚  \\n- **fp16**ï¼šæ˜¯å¦å¯ç”¨16ä½ï¼ˆæ··åˆç²¾åº¦ï¼‰è®­ç»ƒï¼Œç›¸æ¯”é»˜è®¤çš„32ä½è®­ç»ƒèƒ½åŠ é€Ÿè®­ç»ƒå¹¶èŠ‚çœæ˜¾å­˜ã€‚  \\n- **eval_steps**ï¼šæ¯éš”å¤šå°‘è®­ç»ƒæ­¥æ•°è¿›è¡Œä¸€æ¬¡éªŒè¯è¯„ä¼°ï¼Œä»¥ä¾¿è§‚å¯Ÿæ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„è¡¨ç°ã€‚  \\n- **learning_rate**ï¼šè®­ç»ƒæ—¶ä½¿ç”¨çš„å­¦ä¹ ç‡ï¼Œå†³å®šæ¨¡å‹å‚æ•°æ›´æ–°çš„å¹…åº¦ã€‚  \\n- **logging_dir**ï¼šå­˜å‚¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ—¥å¿—ä¿¡æ¯çš„ç›®å½•ï¼Œå¯ä»¥ç”¨æ¥è¿½è¸ªå’Œåˆ†æè®­ç»ƒè¿‡ç¨‹ã€‚  \\n- **logging_steps**ï¼šæ¯éš”å¤šå°‘æ­¥è®°å½•ä¸€æ¬¡è®­ç»ƒæ—¥å¿—ï¼Œå¸®åŠ©ç›‘æ§è®­ç»ƒè¿›åº¦ã€‚  \\n- **run_name**ï¼šè¿™æ¬¡è®­ç»ƒè¿è¡Œçš„åç§°ï¼Œæ–¹ä¾¿åœ¨æ—¥å¿—å’Œç®¡ç†ç•Œé¢ä¸­åŒºåˆ†ä¸åŒçš„å®éªŒã€‚\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "ä¸‹é¢å¯¹æ¯ä¸ªå‚æ•°è¿›è¡Œç®€æ˜é€šä¿—çš„è§£é‡Šï¼š\n",
    "\n",
    "- **output_dir**ï¼šæŒ‡å®šè®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜å¾®è°ƒæ¨¡å‹ã€æ£€æŸ¥ç‚¹ï¼ˆcheckpointï¼‰ç­‰è¾“å‡ºæ–‡ä»¶çš„ç›®å½•ã€‚  \n",
    "- **num_train_epochs**ï¼šæ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­éå†è®­ç»ƒæ•°æ®é›†çš„æ€»è½®æ•°ã€‚  \n",
    "- **per_device_train_batch_size**ï¼šæ¯ä¸ªè®¾å¤‡ï¼ˆä¾‹å¦‚æ¯ä¸ª GPUï¼‰ä¸Šä¸€æ¬¡è®­ç»ƒä½¿ç”¨çš„æ ·æœ¬æ•°ã€‚å¦‚æœæœ‰å¤šä¸ªè®¾å¤‡ï¼Œå®é™…çš„æ‰¹é‡å¤§å°ä¼šä¹˜ä»¥è®¾å¤‡æ•°é‡ã€‚  \n",
    "- **gradient_accumulation_steps**ï¼šæ¯ç´¯è®¡å¤šå°‘æ­¥æ¢¯åº¦åå†è¿›è¡Œä¸€æ¬¡å‚æ•°æ›´æ–°ã€‚è¿™ç›¸å½“äºå¢å¤§äº†æ‰¹é‡å¤§å°ï¼Œæœ‰åŠ©äºåœ¨æ˜¾å­˜è¾ƒå°çš„è®¾å¤‡ä¸Šè®­ç»ƒå¤§æ¨¡å‹ã€‚  \n",
    "- **fp16**ï¼šæ˜¯å¦å¯ç”¨16ä½ï¼ˆæ··åˆç²¾åº¦ï¼‰è®­ç»ƒï¼Œç›¸æ¯”é»˜è®¤çš„32ä½è®­ç»ƒèƒ½åŠ é€Ÿè®­ç»ƒå¹¶èŠ‚çœæ˜¾å­˜ã€‚  \n",
    "- **eval_steps**ï¼šæ¯éš”å¤šå°‘è®­ç»ƒæ­¥æ•°è¿›è¡Œä¸€æ¬¡éªŒè¯è¯„ä¼°ï¼Œä»¥ä¾¿è§‚å¯Ÿæ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„è¡¨ç°ã€‚  \n",
    "- **learning_rate**ï¼šè®­ç»ƒæ—¶ä½¿ç”¨çš„å­¦ä¹ ç‡ï¼Œå†³å®šæ¨¡å‹å‚æ•°æ›´æ–°çš„å¹…åº¦ã€‚  \n",
    "- **logging_dir**ï¼šå­˜å‚¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ—¥å¿—ä¿¡æ¯çš„ç›®å½•ï¼Œå¯ä»¥ç”¨æ¥è¿½è¸ªå’Œåˆ†æè®­ç»ƒè¿‡ç¨‹ã€‚  \n",
    "- **logging_steps**ï¼šæ¯éš”å¤šå°‘æ­¥è®°å½•ä¸€æ¬¡è®­ç»ƒæ—¥å¿—ï¼Œå¸®åŠ©ç›‘æ§è®­ç»ƒè¿›åº¦ã€‚  \n",
    "- **run_name**ï¼šè¿™æ¬¡è®­ç»ƒè¿è¡Œçš„åç§°ï¼Œæ–¹ä¾¿åœ¨æ—¥å¿—å’Œç®¡ç†ç•Œé¢ä¸­åŒºåˆ†ä¸åŒçš„å®éªŒã€‚\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./fine_tuned_models',          # output directory\n",
    "    num_train_epochs=1,              # total number of training epochs\n",
    "    per_device_train_batch_size=1, # batch size per device during training\n",
    "    gradient_accumulation_steps=8, # number of updates steps to accumulate before performing a backward/update pass\n",
    "    fp16=True,                     # GPUWhether to use 16-bit (mixed) precision training instead of 32-bit training.\n",
    "    eval_steps=10,                 # Number of update steps between two evaluations.\n",
    "    learning_rate=5e-5,            # learning rate used for training\n",
    "    logging_dir='./logs',          # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    run_name='deepseek-r1-sft-distill',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "fp16 mixed precision requires a GPU (not 'mps').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_train_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_eval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/project/interview/llm/grokking-llm/venv/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/project/interview/llm/grokking-llm/venv/lib/python3.11/site-packages/transformers/trainer.py:461\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_in_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_accelerator_and_postprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker \u001b[38;5;241m=\u001b[39m TrainerMemoryTracker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mskip_memory_metrics)\n",
      "File \u001b[0;32m~/workspace/project/interview/llm/grokking-llm/venv/lib/python3.11/site-packages/transformers/trainer.py:5099\u001b[0m, in \u001b[0;36mTrainer.create_accelerator_and_postprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5096\u001b[0m     args\u001b[38;5;241m.\u001b[39mupdate(accelerator_config)\n\u001b[1;32m   5098\u001b[0m \u001b[38;5;66;03m# create accelerator object\u001b[39;00m\n\u001b[0;32m-> 5099\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator \u001b[38;5;241m=\u001b[39m \u001b[43mAccelerator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5100\u001b[0m \u001b[38;5;66;03m# some Trainer classes need to use `gather` instead of `gather_for_metrics`, thus we store a flag\u001b[39;00m\n\u001b[1;32m   5101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather_for_metrics\n",
      "File \u001b[0;32m~/workspace/project/interview/llm/grokking-llm/venv/lib/python3.11/site-packages/accelerate/accelerator.py:540\u001b[0m, in \u001b[0;36mAccelerator.__init__\u001b[0;34m(self, device_placement, split_batches, mixed_precision, gradient_accumulation_steps, cpu, dataloader_config, deepspeed_plugin, fsdp_plugin, torch_tp_plugin, megatron_lm_plugin, rng_types, log_with, project_dir, project_config, gradient_accumulation_plugin, step_scheduler_with_optimizer, kwargs_handlers, dynamo_backend, dynamo_plugin, deepspeed_plugins)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnative_amp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmusa\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_xla_available(\n\u001b[1;32m    538\u001b[0m     check_is_tpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    539\u001b[0m ):\n\u001b[0;32m--> 540\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16 mixed precision requires a GPU (not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    541\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_handler\u001b[38;5;241m.\u001b[39mto_kwargs() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;241m=\u001b[39m get_grad_scaler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: fp16 mixed precision requires a GPU (not 'mps')."
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "#model.save_pretrained(model_name)\n",
    "#tokenizer.save_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lora model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './lora_models'\n",
    "# model.save_pretrained(save_path)\n",
    "# tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_save_path = './final_models'\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model = PeftModel.from_pretrained(base_model, save_path)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "model.save_pretrained(final_save_path)\n",
    "tokenizer.save_pretrained(final_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(final_save_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(final_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the meaning of life?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = pipe(prompt, max_length=50, num_return_sequences=1, do_sample=True, temperature=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_text[0]['generated_text'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
